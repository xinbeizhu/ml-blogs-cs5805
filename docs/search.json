[
  {
    "objectID": "posts/Probability-Theory-and-Random-Variables/index.html",
    "href": "posts/Probability-Theory-and-Random-Variables/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "This blog will talk about Probability Theory and Random Variables. Probability theory and random variables are fundamental concepts in statistics and data science. They are used to model and analyze random phenomena and to make predictions based on data.\nFor Probability Theory, to put it simply, imagine we’re about to roll a dice. We’re thinking about what number will come up. It could be 1, 2, 3, 4, 5, or 6. But we’re not sure which one it will be. This uncertainty about what will happen is what Probability Theory deals with. It’s like a mathematical way of studying and measuring uncertainty.\nIn Probability Theory, we talk about the “chance” or “likelihood” of something happening. For example, iweou roll a dice, the chance of getting a 4 is 1 out of 6, because there are 6 sides and only one of them is a 4.\nFor Random Variables, each of these dice numbers is a possible outcome, and we can call them Random Variables.\nIn Probability Theory, we use Random Variables to describe and calculate the chances of different things happening. For example, if we have a bag with 3 red balls and 2 blue balls, and we pick one without looking, Probability Theory helps us figure out the chance of getting a red ball or a blue ball. And these chances are described using Random Variables.\n\nTask Demo\nI’ll use the famous Iris dataset, which contains measurements of iris flowers and their species. I’ll create a model to predict the species based on these measurements. This is also a classification task.\nWhy this classification task demonstrate Probability Theory and Random Variables?\nThe classification task using the Gaussian Naive Bayes classifier. This classifier applies Bayes’ Theorem, a fundamental rule in probability theory. Bayes’ Theorem calculates the probability of an event based on prior knowledge of conditions that might be related to the event.\nIn the dataset, features like petal length and width are considered random variables. They are variables because they can vary from one flower to another, and they are random because each measurement is a result of natural variation.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\n\n# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\n# Create a Gaussian Naive Bayes classifier\nmodel = GaussianNB()\n\n# Train the model\nmodel.fit(X_train, y_train)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Model Accuracy: {accuracy}\")\n\nModel Accuracy: 0.9777777777777777\n\n\n\n# Select a feature for visualization (petal length)\nfeature_idx = 2 # Index of petal length in the dataset\nfeature = X[:, feature_idx]\nspecies = iris.target\n\n# Create a DataFrame for seaborn\ndf = pd.DataFrame({'Petal Length': feature, 'Species': species})\ndf['Species'] = df['Species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n\n# Plot the distributions\nsns.histplot(data=df, x='Petal Length', hue='Species', element='step', stat='density', common_norm=False)\nplt.title('Petal Length Distribution by Species')\nplt.show()\n\nC:\\Users\\zhuhe\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\nC:\\Users\\zhuhe\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\nC:\\Users\\zhuhe\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\nC:\\Users\\zhuhe\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\nC:\\Users\\zhuhe\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\nThis plot illustrates the distribution of petal length across different species. Setosa’s petal length primarily ranges between 1 and 2, Versicolor’s between 3 and 5, and Virginica’s mostly between 5 and 7. There is noticeable overlap in petal lengths for Versicolor and Virginica in the 4-6 range."
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html",
    "href": "posts/Linear and Nonlinear Regression/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Let’s talk about something about linear and nonlinear regression in this blog.\n\nLinear Regression\nLinear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data.\n\nThe dependent variables are what we’re trying to predict or understand. It’s like the “outcome” and is usually on the vertical axis (y-axis) of the graph.\nThe independent variables are the factors we think might affect the outcome. They are on the horizontal axis (x-axis).\n\nEquation: The equation of a linear regression model is typically in the form of \\(y = ax + b\\), where:\n\n\\(y\\) is the dependent variable we’re trying to predict.\n\\(x\\) is the independent variableweu’re using for prediction\n\\(a\\) is the y-intercept (the value of \\(y\\) when \\(x\\) is 0).\n\\(b\\) is the slope of the line, which represents the change in \\(y\\) for a one-unit change in \\(x\\).\n\nUse case: It’s widely used in situations where the relationship between variables is approximately linear, like predicting house prices based on size, predicting salaries based on experience, etc.\n\n\nNonlinear Regression\nNonlinear regression, on the other hand, is used when the data shows a more complex relationship that cannot be accurately described by a straight line. In nonlinear regression:\n\nComplex Relationship: The relationship between the independent and dependent variables is modeled as a nonlinear function. This means the graph of the data points does not form a straight line but could take various shapes (like curves).\nEquation: The equation can take many forms, depending on the nature of the relationship and the data. For example, it could be a polynomial (like \\(y = ax^2 + bx + c\\)), exponential or logistic function.\nUse Cases: Nonlinear regression is used in more complex scenarios where the effect of the independent variables on the dependent variable changes in a non-uniform way. Examples include growth rates of organisms, chemical reaction rates, and market saturation models.\n\n\n\nSummary\nIn summary, linear regression is used for simpler, straight-line relationships, while nonlinear regression is applied to model more complex, curved relationships in data. The choice between the two depends on the nature of the data and the underlying relationship we’re trying to model.\n\n\nTask Demo\nFor the linear and nonlinear regression task, I’d like to predict house prices. I use the Boston Housing dataset from Kaggle.\nLet me import libraries and load the data.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\ndf = pd.read_csv('datasets/housing.csv')\ndf.head()\n\n\n\n\n\n\n\n\nRM\nLSTAT\nPTRATIO\nMEDV\n\n\n\n\n0\n6.575\n4.98\n15.3\n504000.0\n\n\n1\n6.421\n9.14\n17.8\n453600.0\n\n\n2\n7.185\n4.03\n17.8\n728700.0\n\n\n3\n6.998\n2.94\n18.7\n701400.0\n\n\n4\n7.147\n5.33\n18.7\n760200.0\n\n\n\n\n\n\n\nIn this dataset, MEDV stands for “Median value of owner-occupied homes in $1000’s”. Essentially, it represents the median value of houses in various areas of Boston, with the values given in thousands of dollars.\nRM stands for “the average number of rooms per dwelling”.\nLSTAT stands for “percentage of lower status of the population”. In the context of the dataset, a higher LSTAT value is often associated with lower median values of homes (MEDV), as it indicates a higher proportion of lower-income residents in the area.\nPTRATIO stands for “pupil-teacher ratio by town”. It represents the ratio of students to teachers in primary and secondary schools in the neighborhood. A lower PTRATIO might indicate better education resources (i.e., more teachers per student) and is often considered a desirable feature in a residential area.\n\n# Plotting the distribution of MEDV (Median value of owner-occupied homes)\nplt.figure(figsize=(8, 6))\nsns.histplot(df['MEDV'], bins=30, kde=True)\nplt.title('Distribution of MEDV')\nplt.xlabel('Median value of owner-occupied homes (in $1000s)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\nAccording to the plot, most of the medv distribution is around 0.4~0.5, that is, 40,000k to 50,000k.\nLet’s have a look on how the different variables impact the y value.\n\n# Scatter plot of RM (average number of rooms per dwelling) vs MEDV\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x='RM', y='MEDV', data=df)\nplt.title('Relationship between RM and MEDV')\nplt.xlabel('Average Number of Rooms per Dwelling')\nplt.ylabel('Median value of owner-occupied homes (in $1000s)')\nplt.show()\n\n\n\n\nThis image shows that RM and MEDV seem to be in direct proportion to each other.\n\n# Scatter plot of LSTAT (average number of rooms per dwelling) vs MEDV\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x='LSTAT', y='MEDV', data=df)\nplt.title('Relationship between LSTAT and MEDV')\nplt.xlabel('Percentage of Lower Status of the Population')\nplt.ylabel('Median value of owner-occupied homes (in $1000s)')\nplt.show()\n\n\n\n\nThis image shows that LSTAT and MEDV seem to be inversely proportional.\n\n# Scatter plot of PTRATIO (average number of rooms per dwelling) vs MEDV\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x='PTRATIO', y='MEDV', data=df)\nplt.title('Relationship between PTRATIO and MEDV')\nplt.xlabel('Pupil-teacher Ratio by Town')\nplt.ylabel('Median value of owner-occupied homes (in $1000s)')\nplt.show()\n\n\n\n\nIt doesn’t look like there’s a linear relationship between PTRATIO and MEDV.\n\n# Correlation matrix heatmap\nplt.figure(figsize=(12, 10))\nsns.heatmap(df.corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()\n\n\n\n\nThe correlation matrix indicates that MEDV has a stronger correlation with RM.\n\nLinear Regression\nLet’s train our linear regression model and select RM and MEDV for linear regression.\n\nX = df[['RM']]\ny = df['MEDV']\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the Linear Regression model\nlr = LinearRegression()\n\n# Train the model\nlr.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n# Make predictions\ny_pred = lr.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\n\n# Display coefficients\nprint(\"Coefficients:\", lr.coef_)\n\nMean Squared Error: 13362423686.16902\nCoefficients: [176526.28588093]\n\n\n\n# Predictions for the line\ndf['Predictions'] = lr.predict(X)\n\n# Scatter plot with regression line\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='RM', y='MEDV', data=df, label='Data')\nsns.lineplot(x='RM', y='Predictions', data=df, color='red', label='Regression Line')\nplt.title('Linear Regression Line on RM vs MEDV')\nplt.xlabel('Average Number of Rooms per Dwelling (RM)')\nplt.ylabel('Median value of owner-occupied homes (MEDV)')\nplt.legend()\nplt.show()\n\n\n\n\nThe linear model demonstrates a not bad fit.\n\n\nNonlinear Regression\nWe notice that LSTAT and MEDV seem to be inversely proportional. Therefore, let’s select LSTAT and MEDV for nonlinear regression analysis.\n\n# Selecting RM and MEDV for regression\nX = df[['LSTAT']]\ny = df['MEDV']\n\n# Transforming X to 1/X\nX_transformed = 1 / X\n\n# Fit the linear regression model on the transformed data\nnlr = LinearRegression()\nnlr.fit(X_transformed, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n# Predictions\nX_range = np.linspace(X.min(), X.max(), 100)\ny_pred = nlr.predict(1 / X_range)\n\n# Scatter plot with regression line\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='LSTAT', y='MEDV', data=df, label='Data')\nplt.plot(X_range, y_pred, color='red', label='Inverse Regression Line')\nplt.title('Inverse Regression (1/LSTAT vs MEDV)')\nplt.xlabel('Percentage of Lower Status of the Population (LSTAT)')\nplt.ylabel('Median value of owner-occupied homes (MEDV)')\nplt.legend()\nplt.show()\n\n# Model evaluation\ny_pred_train = nlr.predict(X_transformed)\nmse = mean_squared_error(y, y_pred_train)\nr2 = r2_score(y, y_pred_train)\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"R^2 Score: {r2}\")\n\n\n\n\nMean Squared Error: 10081897997.112593\nR^2 Score: 0.6304493154356112\n\n\nThe nonlinear model exhibits a rather good fit."
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "In this blog post, I’ll be discussing classification. Essentially, classification is a technique used in supervised machine learning, where the model is trained to accurately identify the label of any input data it receives. However, this concept is quite straightforward in practice.\nClassification is like sorting things into groups based on what they have in common. Imagine we have a box of toys, and we want to organize them. We might put all the cars in one group, all the dolls in another, and all the blocks in a third group. Each of these groups is like a category.\nIn the same way, in science and in everyday life, we classify things to understand and organize them better. For example, in a library, books are classified or sorted into groups like fiction, non-fiction, science, history, and so on. This helps us find what we’re looking for more easily.\nIn the world of computers and technology, classification is used to teach computers how to recognize different types of things, like pictures of cats and dogs, or emails that are spam or not spam. The computer looks at the features (like shapes, colors, words) of each thing and learns to sort them into the right group.\nHowever, the advanced uses of classification go beyond basic tasks, significantly aiding in complex and vital areas. It’s instrumental in environmental monitoring, where it analyzes satellite and sensor data for tracking changes like deforestation or urban development. In the banking and insurance sectors, it’s crucial for fraud detection, identifying potential fraud to protect financial assets and privacy. Additionally, facial recognition technology, familiar in security and personal devices, relies on classification for identifying individuals.\nBut anyway, classification is just a fancy word for sorting or grouping things based on their features or characteristics.\n\nTask Demo\nFor this blog, I would utilize the classification technology to predict a task related to finance as well, which is whether the loan of a person will be defaulted.\nI adopted a dataset called Loan Default Prediction from Kaggle.\nLet me import libaraies and load the dataset.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n%matplotlib inline\n\n\ndf = pd.read_csv('datasets/Default_Fin.csv')\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 5 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   Index          10000 non-null  int64  \n 1   Employed       10000 non-null  int64  \n 2   Bank Balance   10000 non-null  float64\n 3   Annual Salary  10000 non-null  float64\n 4   Defaulted?     10000 non-null  int64  \ndtypes: float64(2), int64(3)\nmemory usage: 390.8 KB\n\n\n\nax = sns.countplot(data=df,x='Defaulted?')\nax.bar_label(ax.containers[0])\n\n[Text(0, 0, '9667'), Text(0, 0, '333')]\n\n\n\n\n\nIn this plot, ‘0’ represents loans are not defaulted and ‘1’ represents defaulted. We can see that most people’s loan is not defaulted, but there are still people’s loan get defaulted.\n\nsns.scatterplot(data=df,y='Annual Salary',x='Bank Balance',hue='Defaulted?',alpha=0.5);\n\n\n\n\nThis plot shows that annual salary doesn’t really affect loan defaults, as we can see that the orange spots, representing defaults, are spread out evenly up and down. On the other hand, the amount of money people have in their bank accounts does seem to matter. This is because all the orange spots are mostly on the right side. So, it suggests that loans with larger amounts are more likely to be defaulted on.\nLet’s train our prediction model.\n\ndf = df.drop('Index',axis=1).copy()\n\n\nX = df.drop('Defaulted?', axis=1) \ny = df['Defaulted?']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Model training\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier()\n\n\n\n# Model evaluation\npredictions = model.predict(X_test)\nprint(classification_report(y_test, predictions))\n\n              precision    recall  f1-score   support\n\n           0       0.97      0.99      0.98      1931\n           1       0.49      0.26      0.34        69\n\n    accuracy                           0.96      2000\n   macro avg       0.73      0.63      0.66      2000\nweighted avg       0.96      0.96      0.96      2000\n\n\n\nThrough this result, we can tell that the model is very effective in identifying class 0 instances but struggles with class 1 (low recall and precision for class 1). Besides, the high overall accuracy might be misleading due to the imbalance in the dataset. The performance on the minority class (class 1) is a concern.\n\n# Predict probabilities\ny_scores = model.predict_proba(X_test)[:, 1]\n\n# Compute Precision-Recall values\nprecision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n\n# Plot the Precision-Recall curve\nplt.figure()\nplt.plot(recall, precision, marker='.')\nplt.title('Precision-Recall Curve')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.show()\n\n\n\n\nThe PR (Precision-Recall) curve suggests that the model’s performance is suboptimal. For PR curve: - The closer the curve is to the top-right corner of the plot, the better. - A good PR curve will show a gradual decline in precision as recall increases. A steep drop in precision would indicate that the classifier begins to make a lot of false positive errors as it tries to increase its recall. - The area under the PR curve is a useful metric. A larger area under the curve (closer to 1) indicates a better overall performance of the classifier.\nThe poor performance in this task may be due to an imbalance in the dataset between the number of defaulted and non-defaulted loans, combined with a limited range of features used."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ml-blog",
    "section": "",
    "text": "Anomaly/Outlier Detection\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nXinbei Zhu\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\nXinbei Zhu\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\nXinbei Zhu\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nXinbei Zhu\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2023\n\n\nXinbei Zhu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Let’s talk about Clustering in this blog.\nClustering in machine learning is like organizing a big, mixed-up collection of things into groups based on similarities. Imagine we have a bunch of different fruits mixed together and we want to sort them out. Clustering is like grouping apples with apples, bananas with bananas, and so on, based on their features like color, size, or taste.\nIn machine learning, clustering algorithms look at data and try to find these natural groupings. The algorithm scans through the data and tries to figure out which items are similar to each other. It’s like it’s asking: “Does this data point look more like an apple or a banana?”\nThis is useful because sometimes we have a lot of data but we don’t know what categories exist in it. Clustering helps us find these categories automatically, making it easier to understand and use the data. For example, a website could use clustering to group together similar news articles, so it’s easier for readers to find the stories they’re interested in.\n\nTask Demo\nI utilize the 20 Newsfroups Dataset from scikit-learn cluster news articles into groups. Preprocess text data, convert it to TF-IDF vectors, and use an algorithm like K-Means or DBSCAN. Analyze the resulting clusters to see if they make sense in terms of news categories.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Load the dataset\ncategories = ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\nnewsgroups = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n\n# Text preprocessing with TF-IDF\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(newsgroups.data)\n\n# Apply K-Means clustering\nnum_clusters = 4  # As we have chosen 4 categories\nkm = KMeans(n_clusters=num_clusters, random_state=42)\nkm.fit(X)\n\nKMeans(n_clusters=4, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=4, random_state=42)\n\n\n\n# Visualize the results\nlabels = km.labels_\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X.toarray())\n\nplt.figure(figsize=(10, 10))\nfor i in range(num_clusters):\n    cluster = X_pca[labels == i]\n    plt.scatter(cluster[:, 0], cluster[:, 1], label=f'Cluster {i}', edgecolors='black')\nplt.title(\"20 Newsgroups Text Clustering\")\nplt.xlabel(\"PCA Feature 1\")\nplt.ylabel(\"PCA Feature 2\")\nplt.legend()\nplt.show()\n\n\n\n\nThe generated plot using PCA (Principal Component Analysis) to reduce the data to two dimensions gives a visual representation of how the documents are grouped after applying K-Means clustering.\nThis plot reveals that the clusters have minimal overlap, indicating effective separation and good model performance. Besides, the presence of similarities among documents in the green, orange, and red clusters is obvious from their clustering pattern. The dense clustering of green and orange spots implies that their content is quite similar. Conversely, the sparser distribution of blue clusters might suggest a greater diversity in the content of these documents.\nLet’s do it another way by using DBSCAN.\n\n# Standardize features by removing the mean and scaling to unit variance\nX_std = StandardScaler(with_mean=False).fit_transform(X)\n\n# Apply DBSCAN clustering\ndbscan = DBSCAN(eps=0.5, min_samples=5)\nclusters = dbscan.fit_predict(X_std)\n\n\n# Visualize the results\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_std.toarray())\n\nplt.figure(figsize=(10, 10))\nunique_clusters = set(clusters)\nfor cluster in unique_clusters:\n    if cluster == -1:  # Noise points\n        color = 'black'\n    else:\n        color = plt.cm.nipy_spectral(float(cluster) / len(unique_clusters))\n    cluster_points = X_pca[clusters == cluster]\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=[color], edgecolors='black', label=f'Cluster {cluster}')\nplt.title(\"20 Newsgroups Text Clustering with DBSCAN\")\nplt.xlabel(\"PCA Feature 1\")\nplt.ylabel(\"PCA Feature 2\")\nplt.legend()\nplt.show()\n\n\n\n\nThe plot doesn’t display effectively, suggesting that k-means clustering might be a more suitable approach for the 20 News groups dataset."
  },
  {
    "objectID": "posts/Outlier Detection/index.html",
    "href": "posts/Outlier Detection/index.html",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "This blog is about Anomaly/Outlier Detection. Anomaly/Outlier detection, in simple terms, is like finding the odd ones out in a group. Imagine we have a basket of apples, and most of them are red, but a few are green. In this case, the green apples are the outliers because they’re different from the majority, which are red.\nIn the world of data and statistics, outlier detection works in a similar way. It’s the process of identifying data points that are significantly different or unusual compared to the rest of the data. Think of it as spotting something that doesn’t quite fit in with the rest.\nAnomaly or outlier detection in machine learning is a process aimed at identifying data points, events, or observations that deviate significantly from the majority of the data. These anomalies can indicate critical incidents, such as bank fraud, structural defects, medical problems, or errors in text. Anomaly detection is widely used in various fields like finance, healthcare, fault detection, system health monitoring, event detection in sensor networks, and eco-informatics.\nOutlier detection is important because these unusual data points can sometimes indicate a problem or an error, like a mistake in measurement, or they could be a sign of something interesting and worth exploring further, like a new trend.\nThere are three types of anomalies:\n\nPoint Anomalies: A single instance of data is anomalous if it’s too far off from the rest. This is the simplest type of anomaly and is common in fraud detection.\nContextual Anomalies: Anomalies that depend on the context of a situation. This is common in time-series data where a data point might be anomalous in a certain context but not otherwise.\nCollective Anomalies: A collection of data points is anomalous with respect to the entire dataset. This is common in intrusion detection or ecosystem disturbances.\n\n\nTask Demo\nIn this task, I utilize the anomaly/outlier detection technology to detect the fraud in credit card. I adopt the Credit Card Fraud Detection from Kaggle. I use supervised learning to train a outlier detection model and detect the anomaly.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import SMOTE\n\n\n# Load the dataset\ndf = pd.read_csv('dataset/creditcard.csv')\ndf.head()\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\n0\n0.0\n-1.359807\n-0.072781\n2.536347\n1.378155\n-0.338321\n0.462388\n0.239599\n0.098698\n0.363787\n...\n-0.018307\n0.277838\n-0.110474\n0.066928\n0.128539\n-0.189115\n0.133558\n-0.021053\n149.62\n0\n\n\n1\n0.0\n1.191857\n0.266151\n0.166480\n0.448154\n0.060018\n-0.082361\n-0.078803\n0.085102\n-0.255425\n...\n-0.225775\n-0.638672\n0.101288\n-0.339846\n0.167170\n0.125895\n-0.008983\n0.014724\n2.69\n0\n\n\n2\n1.0\n-1.358354\n-1.340163\n1.773209\n0.379780\n-0.503198\n1.800499\n0.791461\n0.247676\n-1.514654\n...\n0.247998\n0.771679\n0.909412\n-0.689281\n-0.327642\n-0.139097\n-0.055353\n-0.059752\n378.66\n0\n\n\n3\n1.0\n-0.966272\n-0.185226\n1.792993\n-0.863291\n-0.010309\n1.247203\n0.237609\n0.377436\n-1.387024\n...\n-0.108300\n0.005274\n-0.190321\n-1.175575\n0.647376\n-0.221929\n0.062723\n0.061458\n123.50\n0\n\n\n4\n2.0\n-1.158233\n0.877737\n1.548718\n0.403034\n-0.407193\n0.095921\n0.592941\n-0.270533\n0.817739\n...\n-0.009431\n0.798278\n-0.137458\n0.141267\n-0.206010\n0.502292\n0.219422\n0.215153\n69.99\n0\n\n\n\n\n5 rows × 31 columns\n\n\n\n\n# Data preprocessing\n# Truncate the dataset, use the first 50,000 rows\ndf_truncated = df.head(50000)\ndf = df_truncated\nX = df.drop('Class', axis=1)\ny = df['Class']\n\n\n# Feature scaling\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Dimensionality Reduction for Visualization\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# Apply DBSCAN\ndbscan = DBSCAN(eps=0.3, min_samples=10)\nclusters = dbscan.fit_predict(X_pca)\n\n\n# Scatter Plot before Model Prediction\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='Paired', marker='o')\nplt.title('DBSCAN Clustering')\nplt.xlabel('PCA Feature 1')\nplt.ylabel('PCA Feature 2')\nplt.colorbar(label='Cluster Label')\n\n&lt;matplotlib.colorbar.Colorbar at 0x1b04bdb5810&gt;\n\n\n\n\n\n\n# Splitting the dataset into the Training set and Test set\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=0)\n\n# Handling imbalanced data\nsm = SMOTE(random_state=2)\nX_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel())\n\n\n# Training the Random Forest model\nclassifier = RandomForestClassifier(n_estimators=100, random_state=0)\nclassifier.fit(X_train_res, y_train_res)\n\nRandomForestClassifier(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(random_state=0)\n\n\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Evaluating the model\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\nprint(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n\n# Plotting the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.matshow(cm, cmap=plt.cm.gray)\nplt.show()\n\nConfusion Matrix:\n[[9965    2]\n [   7   26]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00      9967\n           1       0.93      0.79      0.85        33\n\n    accuracy                           1.00     10000\n   macro avg       0.96      0.89      0.93     10000\nweighted avg       1.00      1.00      1.00     10000\n\nAccuracy Score: 0.9991\n\n\n\n\n\nThe results show that the model performs well. The confusion matrix indicates a low number of false positives and false negatives, and the model also exhibits a high accuracy rate."
  }
]